# -*- coding utf-8 -*-

# This script gathers the routines to extract kilosort/phy (kwikteam) generated units features,
# stored in an instance of DataManager():
# - Sampling rate (30Khz)
# - Spike times in sample units
# - Spikes times in seconds
# - Spikes units, unit corresponding to each spike of Spike times
# - units indexes, array of the unit indexes (one occurence of each)
# - attributed Spike times and Spike samples, lists of n_units np arrays of the form [[unit_idx1, t1, t2...tn], ...] with t1... in seconds or samples
# - Instantaneous Firing rate, lists of n_units np arrays of the form [[unit_idx1, IFR1, IFR2...IFRn], ...]
# - CrossCorrelograms between units
# - Vizualisation tool

# >> How to use it: <<
# Go to the directory which contains the 1) params.py file, 2) spike_times.npy file, 3) spike_clusters.npy file, 4) spike_templates.npy file, 5) templates.npy file and 6) cluster_group.tsv file,
# all being generated by kilosort and exploited/modified by phy (kwikteam).
# Then launch python or ipython from any terminal, and "import CellTypeClassifier". Here it is, you can instanciate DataManager by doing "data = DataManager()".
# Then visualize the Mean Firing Rate, the Instantaneous Firing Rate and the auto/cross correlograms of units thanks to the visualization() method of this instanciation.

# Maxime Beau, 2017-05-10



import numpy as np
from scipy import signal
import random
import time

import scipy
import scipy.stats
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
#matplotlib.style.use('fivethirtyeight')
#matplotlib.style.use('ggplot')
matplotlib.style.use('classic')
import dill
import csv
import os, sys
sys.path.append("/Users/maximebeau/phy")
from phy.utils._types import _as_array # phy, kwikteam
from phy.io.array import _index_of, _unique # phy, kwikteam

def smoothCCG(CCG, win=13, pol=2):
	import scipy.signal as signal
	return signal.savgol_filter(CCG, win, pol)

def plotCCGTEST(data, i, bin_sizeCCG=0.001, window_sizeCCG=0.15, win=13, pol=2):
        import matplotlib.pyplot as plt
        import numpy as np
        idx = np.where(data.units==i)[0][0]
        CCG = data.CCG[idx][idx]
        plt.plot(CCG)
        plt.plot(smoothCCG(CCG, win=win, pol=pol))
        idx1 = np.where(data.usedUnits==i)[0][0]
        x1 = (1./data.extractedFeatures[idx1][1])/bin_sizeCCG
        print ("peakIndexFromCenter: ", x1)
        halfCCG = (len(CCG)-1)/2
        x1+=halfCCG
        x2 = x1
        y1, y2 = 0, CCG.max()
        plt.plot((x1,x2),(y1,y2),'-k')
        plt.show()
        plt.close()

def indices(a, func):
    return [i for (i, val) in enumerate(a) if func(val)]

def _increment(arr, indices):
    """Increment some indices in a 1D vector of non-negative integers.
    Repeated indices are taken into account."""
    arr = _as_array(arr)
    indices = _as_array(indices)
    bbins = np.bincount(indices)
    arr[:len(bbins)] += bbins
    return arr

def _diff_shifted(arr, steps=1):
    arr = _as_array(arr)
    return arr[steps:] - arr[:len(arr) - steps]

def _create_correlograms_array(n_units, winsize_bins):
	'''3D matrix of n_units x n_units x window size in amount of bins'''
	return np.zeros((n_units, n_units, winsize_bins // 2 + 1), dtype=np.int32)

def _symmetrize_correlograms(correlograms):
    """Return the symmetrized version of the CCG arrays."""

    n_units, _, n_bins = correlograms.shape
    assert n_units == _

    # We symmetrize c[i, j, 0].
    # This is necessary because the algorithm in correlograms()
    # is sensitive to the order of identical spikes.
    correlograms[..., 0] = np.maximum(correlograms[..., 0],
                                      correlograms[..., 0].T)

    sym = correlograms[..., 1:][..., ::-1]
    sym = np.transpose(sym, (1, 0, 2))

    return np.dstack((sym, correlograms))

def load_DM(data, filename):
	'''Arguments: "data"=NameOfDataManager, "filename" has to be of the form "xxxxxxx.pkl".'''
	with open(filename, 'wb') as Input:
		data = dill.load(Input)

class DataManager():
	'''Has to be informed of the directory of kilosort output (files reshaped by phy).
		The directory can be provided as argument. If no argument is provided, the used directory is the current working directory.

		methods() -> attributes:

		chdir() -> .__dir__: directory to seek for data and generate plots.

		load_sampleRate() -> .sample_rate: Sampling rate (30Khz)

		load_spsamples() -> .spike_samples: Spike times in sample units

		calcul_sptimes() -> .spike_times: Spikes times in seconds

		load_spunits() -> .spike_clusters: Spikes units, unit corresponding to each spike of Spike times

		extract_cIds() -> .units, .spike_clusters_i: units numbers (unique occurence) and indexes in the array

		attribute_spikeSamples_Times() -> .attributed_spikeSamples, .attributed_spikeSamplesDic, .attributed_spikeTimes, .attributed_spikeTimesDic: 
			attributed Spike times and Spike samples,
			lists of n_units np arrays of the form [np.array([unit_idx1, t1, t2...tn]), ...]
			or dictionnaries of the form {unitIndex: np.array([t1, t2...tn]), ...}
			with t1... in seconds or samples.

		attribute_spikeTemplates() -> .attributed_spikeTemplatesIdx, self.attributed_spikeTemplates, self.attributed_spikeTemplatesDic: 
			attributed Spike templates, 
			lists of n_units np arrays of the form [np.array([unit_idx1, v1, v2, v3... vn_timepoints]), ...]
			or dictionnaries of the form {unitIndex: np.array([v1, v2... vn_timepoints]), ...}
			with v1... being the units templates points (np arrays storing n_timepoints values).

		InstFR() -> .IFR: Instantaneous Firing rate, lists of n_units np arrays of the form [[unit_idx1, IFR1, IFR2...IFRn], ...]

		MeanFR() -> .MFR: Mean Firing rate, lists of n_units np arrays of the form [[unit_idx1, MFR], ...]

		CCG() -> .correlograms: all crossCorrelograms in a n_units x n_units x winsize_bins matrix.

		visualize() -> Visualization tool of DataManager attributes.
			Argument1: list of units whose features need to be visualized (int or float). [unit1, unit2...]
			Argument2: list of features to visualize (str). Can contain "IFR": Instantaneous Firing Rate, "MFR": Mean Firing Rate, "CCG": CrossCorreloGrams, "WVF": weighted averaged templates.

		save() -> saving a DataManager() instance. Argument: filename, has to be of the form "xxxxxxx.pkl".'''

	def __init__(self, directory=None):
		if directory==None:
			directory = os.getcwd()
		elif directory==1:
			directory='/Volumes/DK_students1/2017-04-08'
		elif directory==2:
			directory='/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/debugCTC'
		os.chdir(directory)
		self.__dir__ = directory

	def chdir(self, directory="/Volumes/DK_students1/2017-04-08"):
		'''chdir() -> .__dir__: directory to seek for data and generate plots.'''
		os.chdir(directory)
		self.__dir__ = directory

	def load_sampleRate(self):
		'''load_sampleRate() -> .sample_rate: Sampling rate (30Khz)'''
		try:
			print("Sample rate already loaded: ", self.sample_rate)
			if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			import params
			self.sample_rate = params.sample_rate
			print("Sample rate loaded.")
		return self.sample_rate

	def load_spsamples(self):
		'''load_spsamples() -> .spike_samples: Spike times in sample units'''
		try:
			print("Spike samples already loaded. Array shape:", self.spike_samples.shape)
			if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.spike_samples = np.load("spike_times.npy")
			self.spike_samples = self.spike_samples.flatten()
			self.spike_samples = np.asarray(self.spike_samples, dtype=np.int64)
			print("Spike samples loaded.")
		return self.spike_samples

	def calcul_sptimes(self):
		'''calcul_sptimes() -> .spike_times: Spikes times in seconds'''
		try:
			print("Spike times already calculated. Array shape:", self.spike_times.shape)
			if input(" -- Calcul again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.load_sampleRate()
			self.load_spsamples()
			self.spike_times = self.spike_samples/self.sample_rate
			self.spike_times = np.asarray(self.spike_times, dtype=np.float64)
			print("Spike times calculated.")
		return self.spike_times

	def load_spunits(self):
		'''load_spunits() -> .spike_clusters: Spikes units, corresponding to each spike of Spike times'''
		try:
			print("Spike units already loaded. Array shape:", self.spike_clusters.shape)
			if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.spike_clusters = np.load("spike_clusters.npy")
			print("Spike units loaded.")
		return self.spike_clusters

	def extract_cIds(self):
		'''extract_cIds() -> .units, .spike_clusters_i: units numbers (unique occurence) and indexes in the array'''
		try:
			print("unit Ids already extracted. Array shape:", self.units.shape)
			if input(" -- Extract again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.load_spunits()
			self.units = _unique(self.spike_clusters)
			self.spike_clusters_i = _index_of(self.spike_clusters, self.units)
			self.units = np.asarray(self.units, dtype=np.int64)

			self.goodUnits = np.array([])
			with open("cluster_group.tsv", "r") as readingTSV: 
				readingTSV = csv.DictReader(readingTSV, delimiter="\t")
				for row in readingTSV:
					if row["group"]=="good":
						self.goodUnits = np.append(self.goodUnits, np.array([int(row["cluster_id"])]))
			#self.spike_clusters_i = np.asarray(self.spike_clusters_i, dtype=np.float64)
			print("units Ids extracted.")
		return self.units, self.spike_clusters_i

	def load_sptemplates(self):
		'''load_sptemplates() -> .spike_templates: Spike templates, corresponding to each spike of Spike times'''
		try:
			print("Spike templates already loaded. Array shape:", self.spike_templates.shape)
			if input(" -- Load again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.spike_templates = np.load("spike_templates.npy")
			self.spike_templates = self.spike_templates.flatten()
			self.spike_templates = np.asarray(self.spike_templates, dtype=np.int64)
			print("Spike templates loaded.")
			self.templates = np.load("templates.npy")
			self.templates = np.asarray(self.templates, dtype=np.float64)
			print("Templates loaded.")
		return self.spike_templates, self.templates

	def attribute_spikeSamples_Times(self):
		'''attribute_spikeSamples_Times() -> .attributed_spikeSamples, .attributed_spikeSamplesDic, .attributed_spikeTimes, .attributed_spikeTimesDic:
		attributed Spike times and Spike samples, lists of n_units np arrays of the form 
		[[unit_idx1, t1, t2...tn], ...] 
		with t1... in seconds or samples'''
		try:
			print("Spike samples and times already attributed. List length:", len(self.attributed_spikeTimes))
			if input("Attribute again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.calcul_sptimes()
			self.extract_cIds()
			self.attributed_spikeSamples = []
			self.attributed_spikeSamplesDic = {}
			self.attributed_spikeTimes = []
			self.attributed_spikeTimesDic = {}
			for UNIT in self.units:
				d = np.where(self.spike_clusters==UNIT)
				unit = np.array([UNIT], dtype=np.int64)
				arr1 = np.append(unit, self.spike_samples[d])
				self.attributed_spikeSamples.append(arr1)
				self.attributed_spikeSamplesDic[UNIT]=self.spike_samples[d]
				arr2 = np.append(unit, self.spike_samples[d]*1./self.sample_rate)
				self.attributed_spikeTimes.append(arr2)
				self.attributed_spikeTimesDic[UNIT]=self.spike_samples[d]*1./self.sample_rate

			#self.attributed_spikeTimes = [x.copy() for x in self.attributed_spikeSamples]
			#self.attributed_spikeTimesDic = self.attributed_spikeSamplesDic.copy()
			#for i, x in enumerate(self.attributed_spikeTimes):
			#	self.attributed_spikeTimes[i][1:] = x[1:]/self.sample_rate
			#for key, val in self.attributed_spikeTimesDic.items():
			#	self.attributed_spikeTimesDic[key]=val/self.sample_rate

			print("Spike samples and times attributed.")
		return self.attributed_spikeSamples, self.attributed_spikeSamplesDic, self.attributed_spikeTimes, self.attributed_spikeTimesDic

	def attribute_spikeTemplates(self):
		'''attribute_spikeTemplates() -> .attributed_spikeTemplatesIdx, self.attributed_spikeTemplates, self.attributed_spikeTemplatesDic: 
		attributed Spike templates, lists of n_units np arrays of the form 
		[[unit_idx1, t1, t2, t1, t3...tn], ...] 
		with t1... being the templates (np arrays storing n_timepoints values)'''
		try:
			print("Spike templates attributed. List length:", len(self.attributed_spikeTemplates))
			if input(" -- Attribute again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.load_sptemplates()
			self.extract_cIds()
			# Attribute template indexes and counts to each unit
			self.attributed_spikeTemplatesIdx = {}
			for UNIT in self.units:
				d = np.where(self.spike_clusters==UNIT)
				unique, counts = np.unique(self.spike_templates[d], return_counts=True)
				self.attributed_spikeTemplatesIdx[UNIT]=dict(zip(unique, counts))
			# attribute the templates themselves (weighted average) on the max amplitude channel
			self.attributed_spikeTemplates = []
			self.attributed_spikeTemplatesDic = {}
			for UNIT, UNITTemps in self.attributed_spikeTemplatesIdx.items():
				template = np.zeros(len(self.templates[0]))
				totalCounts = 0
				for tempIdx, tempCounts in UNITTemps.items():
					channelAbsMax = np.where(abs(self.templates[tempIdx-1]) == abs(self.templates[tempIdx-1]).max())[1][0] # Find the channel where the template has the maximum absolute amplitude
					template += np.array([self.templates[tempIdx-1][i][channelAbsMax] for i in range(len(self.templates[tempIdx-1]))], dtype=np.float64)*tempCounts # self.templates[0] accounts for template #1
					totalCounts+=tempCounts
				template/=totalCounts
				unit = np.array([UNIT], dtype=np.int64)
				arr = np.append(unit, template)
				self.attributed_spikeTemplates.append(arr)
				self.attributed_spikeTemplatesDic[UNIT]=template


			print("Spikes templates attributed.")
		return self.attributed_spikeTemplatesIdx, self.attributed_spikeTemplates, self.attributed_spikeTemplatesDic

	def InstFR(self, binsize = 0.002, sd = 10):
		'''InstFR() -> .IFR: Instantaneous Firing rate, lists of n_units np arrays of the form [[unit_idx1, IFR1, IFR2...IFRn], ...].
		Binsize is in seconds.'''
		try:
			print("IFR already calculated. List length:", len(self.IFR))
			if input(" -- Calcul again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.attribute_spikeSamples_Times()
			gaussian = signal.gaussian(90, sd)
			self.IFRhist = self.attributed_spikeSamples.copy()
			self.IFR = self.attributed_spikeSamples.copy()
			self.IFRhistDic = {}
			self.IFRDic = {}
			binsize*=self.sample_rate
			binEdges = np.arange(0, self.spike_samples[-1], binsize)
			for i, x in enumerate(self.attributed_spikeSamples):
				hist = np.histogram(x[1:], binEdges)
				conv = np.convolve(hist[0], gaussian)
				UNIT = x[0]
				self.IFRhist[i] = np.append(UNIT, hist[0])
				self.IFR[i] = np.append(UNIT, conv)
				self.IFRhistDic[UNIT]=hist[0]
				self.IFRDic[UNIT]=conv
			print("Instantaneous Firing rates calculated.")
			self.IFRhist = np.asarray(self.IFRhist)
			self.IFR = np.asarray(self.IFR)
		return self.IFR, self.IFRDic

	def MeanFR(self):
		'''MeanFR() -> .MFR: Mean Firing rate, lists of n_units np arrays of the form [np.array([unit_idx1, MFR]), ...]'''
		try:
			print("MFR already calculated. List length:", len(self.MFR))
			if input(" -- Calcul again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.attribute_spikeSamples_Times()
            #Approximate length of the whole recording, in seconds
			recordLen = float(self.spike_times[-1])           
			# Calculate MFR
			self.MFR = []
			self.MFRDic = {}
			for i, x in enumerate(self.attributed_spikeTimes):
				# Remove 1min shanks without any spikes
				shanksSize = 60 # seconds
				recordLenWithSpikes = 0
				for t in range(int(recordLen/shanksSize)):
					shank = (t*shanksSize, (t+1)*shanksSize)
					spikesInShank = np.where(np.logical_and(self.attributed_spikeTimes[i][1:]>=shank[0], self.attributed_spikeTimes[i][1:]<=shank[1]))
					if spikesInShank[0].any():
						recordLenWithSpikes+=shanksSize
					if recordLenWithSpikes == int(recordLen/shanksSize)*shanksSize:
						recordLenWithSpikes = recordLen
				self.MFR.append(np.array([self.attributed_spikeTimes[i][0], float(len(self.attributed_spikeTimes[i][1:]))/recordLenWithSpikes]))
				self.MFRDic[self.attributed_spikeTimes[i][0]] = float(len(self.attributed_spikeTimes[i][1:]))/recordLenWithSpikes
			print("Mean firing rate calculated.")
			self.MFR = np.asarray(self.MFR)
		return self.MFR, self.MFRDic

	def CrossCG(self, bin_size=0.001, window_size=0.15, symmetrize=True, normalize = True):
		'''CCG() -> .CCG: all crossCorrelograms in a n_units x n_units x winsize_bins matrix.By default, bin_size=0.001 and window_size=0.080, in seconds.'''
		try:
			print("CrossCorrelograms already computed.", len(self.correlograms))
			if input(" -- Compute again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.calcul_sptimes()
			self.extract_cIds()
			assert self.sample_rate > 0.
			assert np.all(np.diff(self.spike_times) >= 0), ("The spike times must be increasing.")
			assert self.spike_samples.ndim == 1
			assert self.spike_samples.shape == self.spike_clusters.shape
			bin_size = np.clip(bin_size, 1e-5, 1e5)  # in seconds
			binsize = int(self.sample_rate * bin_size)  # in samples
			assert binsize >= 1

			window_size = np.clip(window_size, 1e-5, 1e5)  # in seconds
			winsize_bins = 2 * int(.5 * window_size / bin_size) + 1
			assert winsize_bins >= 1
			assert winsize_bins % 2 == 1


			n_units = len(self.units)

			# Shift between the two copies of the spike trains.
			shift = 1

		    # At a given shift, the mask precises which spikes have matching spikes
		    # within the correlogram time window.
			mask = np.ones_like(self.spike_samples, dtype=np.bool)

			self.correlograms = _create_correlograms_array(n_units, winsize_bins)
			print(" - CCG bins: ", winsize_bins)

		    # The loop continues as long as there is at least one spike with
		    # a matching spike.
			while mask[:-shift].any():
		        # Number of time samples between spike i and spike i+shift.
				spike_diff = _diff_shifted(self.spike_samples, shift)

		        # Binarize the delays between spike i and spike i+shift.
				spike_diff_b = spike_diff // binsize

		        # Spikes with no matching spikes are masked.
				mask[:-shift][spike_diff_b > (winsize_bins // 2)] = False

		        # Cache the masked spike delays.
				m = mask[:-shift].copy()
				d = spike_diff_b[m]

		        # # Update the masks given the units to update.
		        # m0 = np.in1d(spike_clusters[:-shift], units)
		        # m = m & m0
		        # d = spike_diff_b[m]
				d = spike_diff_b[m]

		        # Find the indices in the raveled correlograms array that need
		        # to be incremented, taking into account the spike units.
				indices = np.ravel_multi_index((self.spike_clusters_i[:-shift][m], self.spike_clusters_i[+shift:][m], d), self.correlograms.shape)

		        # Increment the matching spikes in the correlograms array.
				_increment(self.correlograms.ravel(), indices)

				shift += 1
			
			# Remove ACG peaks
			self.correlograms[np.arange(n_units),
	                 np.arange(n_units),
	                 0] = 0

			if symmetrize==True:
				self.correlograms = _symmetrize_correlograms(self.correlograms)
			print("CrossCorrelograms computed.")
			
			if normalize==True:
				self.correlograms = np.apply_along_axis(lambda x: x/np.sum(x) if np.sum(x)!=0 else x, 2, self.correlograms)


			self.CCG = self.correlograms

		return self.CCG

	def InterSI(self, bin_size=0.0005, window_size=0.2, normalize = True):
		'''InterSI() -> .ISI: all interspike interval histograms in a list of the form [np.array([unit_idx1, ISIcounts1, ISIcounts2...]), ...]. By default, bin_size=0.0005 in seconds.'''
		try:
			print("InterSpikeIntervals already computed.", len(self.ISI))
			if input(" -- Compute again? Dial <anything> for yes, <enter> for no: "):
				raise
		except:
			self.attribute_spikeSamples_Times()
			self.ISI = []
			self.ISIDic = {}
			self.ISIparams = []
			self.ISIparamsDic = {}
			self.ISIList = []
			self.ISIListDic = {}
			binEdges = np.arange(0, window_size+bin_size, bin_size) # in seconds
			alpha = getattr(scipy.stats, 'alpha')
			for i, x in enumerate(self.attributed_spikeTimes):
				ISIlist = list(np.diff(self.attributed_spikeTimes[i][1:]))
				ISIhist = np.histogram(ISIlist, binEdges)
				ISIparams = np.array([np.mean(ISIlist), np.var(ISIlist), scipy.stats.skew(ISIlist)])
				self.ISI.append(np.append(np.array([x[0]]), ISIhist[0]))
				self.ISIDic[x[0]]=ISIhist[0]
				self.ISIparams.append(np.append(np.array([x[0]]), ISIparams))
				self.ISIparamsDic[x[0]]=ISIparams
				self.ISIList.append(np.append(np.array([x[0]]), ISIlist))
				self.ISIListDic[x[0]]=ISIlist
			if normalize==True:
				self.ISI = np.apply_along_axis(lambda x: x/np.sum(x) if np.sum(x)!=0 else x, 1, self.ISI)


		return self.ISI, self.ISIDic


	def extractFeatures(self, featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.001, window_sizeCCG=0.15, bin_sizeISI=0.0005):
		'''extractFeatures() -> .extractedFeatures, creates a numy array of shape (n_units, n_features) compatible with scikit-learn uniting/classifying.
		.extractedFeatures[:,0]: MFR
		.extractedFeatures[:,1]: 
		.extractedFeatures[:,2]: 
		.extractedFeatures[:,3]: 
		.extractedFeatures[:,4]: 
		.extractedFeatures[:,5]: 
		.extractedFeatures[:,6]: 
		.extractedFeatures[:,7]: 
		.extractedFeatures[:,8]: 
		.extractedFeatures[:,9]: '''

		features = []

		self.nanUnits = np.array([])

		if 'MFR' in featuresList:
			self.MeanFR()
			self.MFRf0 = {}
			for unitIdx, unit in enumerate(self.units):
				if unit in self.goodUnits:
					self.MFRf0[unit]=self.MFRDic[unit]
			features.append(self.MFRf0)


		if 'CCG' in featuresList:
			self.CrossCG(bin_size=bin_sizeCCG, window_size=window_sizeCCG)

			self.CCGf0, self.CCGf1, self.CCGf2, self.CCGf3, self.CCGf4, self.CCGf5 = {}, {}, {}, {}, {}, {}
			for unitIdx, unit in enumerate(self.units):
				if unit in self.goodUnits:
					CCG = self.CCG[unitIdx][unitIdx]
					halfCCG = int((len(CCG)-1)/2)
					smoothCCG = signal.savgol_filter(CCG, 13, 2)
					diffCCG = np.diff(smoothCCG)/bin_sizeCCG

					if abs(np.where(CCG==CCG.max())[0][1]-halfCCG)<=(window_sizeCCG/bin_sizeCCG)*0.95: # If the maximum is not on the extremity = problem
						for i in range(halfCCG-1): # i max = halfCCG-2
							y1 = diffCCG[halfCCG+i] # Needs to start at diffCCG[halfCCG] = diff between CCG[halfCCG] (center) and CCG[halfCCG+1] (right after center)
							y2 = diffCCG[halfCCG+i+1] # Needs to end at diffCCG[halfCCG+halfCCG-1]
							print("Unit:", unit," - y1:", y1, " - y2:", y2, " - i:", i)
							if y1>0 and y2<0:
								# peak ZONE found thanks to the smoothed ACG
								print("PEAK ZONE FOUND: bin half+", i+1)
								peakWinHalf = 10
								peakWindow = np.array([CCG[halfCCG+i+1+j] for j in np.arange(-peakWinHalf,peakWinHalf+1,1)])
								# peak INDEX found thanks to the not smoothed ACG, more accurate
								peakIndexFromLeft = (halfCCG+i+1 #Index of smoothed ACG peak
														+ np.where(peakWindow==peakWindow.max())[0][0]-peakWinHalf) # Index of unsmoothed ACG peak
								peakIndexFromCenter = abs(peakIndexFromLeft - halfCCG)
								self.CCGf0[unit] = 1./(peakIndexFromCenter*bin_sizeCCG)
								print("peakIndexFromCenter:", peakIndexFromCenter)
								break
					else:
						self.nanUnits = np.append(self.nanUnits, np.array([unit]))
						self.CCGf0[unit] = np.nan

					if CCG[halfCCG:].sum()!=0:
						self.CCGf1[unit] = np.divide(CCG[halfCCG:int(halfCCG+halfCCG*1./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the first 1/5th of the half CCG
						self.CCGf2[unit] = np.divide(CCG[int(halfCCG+halfCCG*1./5):int(halfCCG+halfCCG*2./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the second 1/5th of the half CCG
						self.CCGf3[unit] = np.divide(CCG[int(halfCCG+halfCCG*2./5):int(halfCCG+halfCCG*3./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the third 1/5th of the half CCG
						self.CCGf4[unit] = np.divide(CCG[int(halfCCG+halfCCG*3./5):int(halfCCG+halfCCG*4./5)].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the fourth 1/5th of the half CCG
						self.CCGf5[unit] = np.divide(CCG[int(halfCCG+halfCCG*4./5):].sum(), CCG[halfCCG:].sum()) # % of the area under the curve of the half CCG contained in the last 1/5th of the half CCG
					else:
						self.nanUnits = np.append(self.nanUnits, np.array([unit]))
						self.CCGf1[unit] = np.nan
						self.CCGf2[unit] = np.nan
						self.CCGf3[unit] = np.nan
						self.CCGf4[unit] = np.nan
						self.CCGf5[unit] = np.nan

			features.append(self.CCGf0)
			features.append(self.CCGf1)
			features.append(self.CCGf2)
			features.append(self.CCGf3)
			features.append(self.CCGf4)
			features.append(self.CCGf5)
		

		if "ISI" in featuresList:
			self.InterSI(bin_size=bin_sizeISI)
			self.ISIf0, self.ISIf1, self.ISIf2, self.CCGf3, self.CCGf4, self.CCGf5 = {}, {}, {}, {}, {}, {}
			for unitIdx, unit in enumerate(self.units):
				if unit in self.goodUnits:
					self.ISIf0[unit] = 1./self.ISIparamsDic[unit][0] # Mean of the ISI distribution (moment 1) ~ 1/mean firing rate
					self.ISIf1[unit] = self.ISIparamsDic[unit][1] # Variance of the ISI ditribution (moment 2)
					self.ISIf2[unit] = self.ISIparamsDic[unit][2] # Skewness of the ISI distribution (moment 3)
			features.append(self.ISIf0)
			features.append(self.ISIf1)
			features.append(self.ISIf2)

		# if 'WVF' in featuresList:
			# self.attribute_spikeTemplates()
			# for unitIdx, unit in enumerate(self.units):

		self.nanUnits = np.unique(self.nanUnits)
		self.usedUnits = np.array([i for i in self.goodUnits if i not in self.nanUnits])
		n_units = len(self.usedUnits)
		n_features = len(features)
		self.extractedFeatures = np.zeros((n_units, n_features))
		for unitIdx, unit in enumerate(self.usedUnits):
			self.extractedFeatures[unitIdx] += np.array([feat[unit] for feat in features])


		return self.extractedFeatures



	def visualize(self, unitsList=None, featuresList=None, SHOW=True, bin_sizeIFR=0.002, bin_sizeCCG=0.001, window_sizeCCG=0.15, bin_sizeISI=0.0005, window_sizeISI=0.2, plotType='bar', normalizeCCG = True):
		'''visualize() -> Visualization tool.
		Argument1: list of units whose features need to be visualized (int or float). [unit1, unit2...]
		Argument2: list of features to visualize (str). Can contain "IFR": Instantaneous Firing Rate, "MFR": Mean Firing Rate, "CCG": CrossCorreloGrams, "ISI" InterSpikeInterval, "WVF": weighted averaged templates.'''
		EXIT = False
		while 1:
			self.extract_cIds()

			if unitsList == None or unitsList == []:
				unitsList = []

				while 1:
					idx = input("\n\nPlease dial a unit index ; dial <d> if you are done: ")

					if idx == "d":
						break
					else:
						try:
							idx = int(idx) # input() only returns strings
							if idx in self.units:
								if idx in self.goodUnits:
									print("\nThis unit is classified as good.")
								if idx not in self.goodUnits:
									print("\nThis unit is not classified as good.")
								unitsList.append(idx)
							else:
								print("\nThis index is not detected in the unit indexes of this directory's data. Try another one.")
						except:
							print("\nYou must dial a floatting point number or an integer.")

				if unitsList==[]:
					print("\nYou didn't provide any unit. Not very wise, young (wo)man.")
					EXIT = True

			if (featuresList == None or featuresList == []) and EXIT == False:
				featuresList = []

				while 1:
					idx = input("\n\nPlease dial a feature to visualize - <MFR>, <IFR>, <CCG>, <ISI> or <WVF>; dial <d> if you are done: ")
					
					if idx == "d":
						break
					if idx == 'MFR' or idx == 'IFR' or idx == 'CCG' or idx == 'ISI' or idx == 'WVF':
						featuresList.append(idx)
					else:
						print("\nYou must dial <MFR>, <IFR>, <CCG>, <ISI> or <WVF>.")

				if featuresList==[]:
					print("\nYou didn't provide any feature. Not very wise, young (wo)man.")
					EXIT = True

			if EXIT==True:
				print("\n\nGoodbye, thanks for using Max's tools.")
				break


			print("\n\n--> Units to visualize: ", unitsList, "\n\n--> Features displayed: ", featuresList, "\n\n")

			if "MFR" in featuresList:
				self.MeanFR()
				MFRList = []
				for i in unitsList:
					MFRidx = np.where(self.units==i)[0][0]
					MFRList.append(self.MFR[MFRidx][1])
				unitsListStr = [str(i) for i in unitsList]
				assert len(unitsListStr) == len(MFRList)

				dfMFR = pd.DataFrame(data=MFRList, index=unitsListStr, columns=["Mean Firing rate (Hz)"])
				axMFR = dfMFR.plot.bar(legend=False, width=0.1, linewidth=2, color=(0./255, 204./255, 0./255), edgecolor='k')
				for p in axMFR.patches:
					axMFR.annotate('~'+str(round(p.get_height(), 3)), (p.get_x() * 1.005, p.get_height() * 1.005))
				figMFR = axMFR.get_figure()
				if not os.path.exists(self.__dir__+'/visMFRs'):
					os.makedirs(self.__dir__+'/visMFRs')
				figMFRpath = self.__dir__+'/visMFRs'+'/MFR'
				for i in unitsListStr:
					figMFRpath+=', '
					figMFRpath+=i
				figMFR.savefig(figMFRpath+'.eps')
				figMFR.savefig(figMFRpath+'.png')
				
				self.dfMFR = dfMFR
				break


			if "IFR" in featuresList:
				self.InstFR(bin_size = bin_sizeIFR, window_size = window_sizeIFR)
				IFRDic = {}
				for i in unitsList:
					IFRidx = np.where(self.units==i)[0][0]
					IFRDic[self.IFR[IFRidx][0]] = pd.Series(self.IFR[IFRidx][1:].tolist(), index=np.arange(len(self.IFR[IFRidx][1:]))*(bin_sizeIFR))
				unitsListStr = [str(i) for i in unitsList]

				dfIFR = pd.DataFrame(IFRDic)
				axIFR = dfIFR.plot.area(stacked=False, x_compat=True)
				axIFR.set_xlabel('Recording time (ms)', fontsize=6)
				axIFR.set_xlabel('Instantaneous Firing Rate ('+str(1./bin_size)+'Hz)', fontsize=6)
				figIFR = axIFR.get_figure()
				if not os.path.exists(self.__dir__+'/visIFRs'):
					os.makedirs(self.__dir__+'/visIFRs')
				figIFRpath = self.__dir__+'/visIFRs'+'/IFR'
				unitsListStr = [str(i) for i in unitsList]
				for i in unitsListStr:
					figIFRpath+=', '
					figIFRpath+=i
				figIFR.savefig(figIFRpath+'.eps')
				figIFR.savefig(figIFRpath+'.png')
				
				self.dfIFR = dfIFR
				break


			if "CCG" in featuresList:
				self.CrossCG(bin_size=bin_sizeCCG, window_size=window_sizeCCG, normalize = normalizeCCG)
				plotsxticks = np.arange(-window_sizeCCG*1000/2, window_sizeCCG*1000/2+bin_sizeCCG*1000, bin_sizeCCG*1000)

				CCGDic_df = {} # Need to create a 3D dataset - panel from pandas. Made from dic of dataframes.
				for i in unitsList:
					CCGDic_sr = {} # Dataframes made from dic of series.
					CCGidxI = np.where(self.units==i)[0][0]
					for j in unitsList:
						CCGidxJ = np.where(self.units==j)[0][0]
						CCGDic_sr[j] = pd.Series(self.CCG[CCGidxI][CCGidxJ].tolist(), index=plotsxticks)
					CCGDic_df[i]=pd.DataFrame(CCGDic_sr)

				pnCCG = pd.Panel(CCGDic_df)
				pnCCG = pnCCG.swapaxes('minor_axis', 'major_axis')
				dfCCG = pnCCG.to_frame()

				'''dfCCG = dfCCG.unstack(level=0)
				axCCG = dfCCG.plot(kind='bar', subplots=True)'''


				figCCG, CCGaxis = plt.subplots(len(unitsList), len(unitsList))
				if len(unitsList)==1:
					colorFlag = 0
					for i, x in enumerate(unitsList):
						for j, y in enumerate(unitsList):
							if i == j:
								if colorFlag%5 == 0:
									color = (51./255, 153./255, 255./255)
								elif colorFlag%5 == 1:
									color = (255./255, 0./255, 0./255)
								elif colorFlag%5 == 2:
									color = (255./255, 255./255, 0./255)
								elif colorFlag%5 == 3:
									color = (0./255, 153./255, 0./255)
								elif colorFlag%5 == 4:
									color = (102./255, 0./255, 204./255)
								colorFlag+=1
							else:
								color = (0./255, 0./255, 0./255)
							if plotType == 'bar':
								CCGaxis.bar(plotsxticks,dfCCG[x][y], color = color, linewidth=0.5)
							elif plotType == 'line':
								CCGaxis.plot(plotsxticks,dfCCG[x][y], color = color)
							CCGaxis.set_xlabel('dt (ms)', fontsize=6)
							CCGaxis.set_xlim((-window_sizeCCG*1000/2,window_sizeCCG*1000/2))
							if normalizeCCG:
								CCGaxis.set_ylabel('counts - normalized ('+str(np.sum(dfCCG[x][y]))+')', fontsize=6)
							else:
								CCGaxis.set_ylabel('counts', fontsize=6)
							CCGaxis.tick_params(labelsize=4, color='k')
							CCGaxis.set_title(str(x)+'-'+str(y),fontsize=10)
				else:
					colorFlag = 0	
					for i, x in enumerate(unitsList):
						for j, y in enumerate(unitsList):
							if i == j:
								if colorFlag%5 == 0:
									color = (51./255, 153./255, 255./255)
								elif colorFlag%5 == 1:
									color = (255./255, 0./255, 0./255)
								elif colorFlag%5 == 2:
									color = (255./255, 255./255, 0./255)
								elif colorFlag%5 == 3:
									color = (0./255, 153./255, 0./255)
								elif colorFlag%5 == 4:
									color = (102./255, 0./255, 204./255)
								colorFlag+=1
							else:
								color = (0./255, 0./255, 0./255)
							if plotType == 'bar':
								CCGaxis[i][j].bar(plotsxticks,dfCCG[x][y], color = color, linewidth=0.5)
							elif plotType == 'line':
								CCGaxis[i][j].plot(plotsxticks,dfCCG[x][y], color = color)
							CCGaxis[i][j].set_xlabel('dt (ms)', fontsize=6)
							CCGaxis[i][j].set_xlim((-window_sizeCCG*1000/2,window_sizeCCG*1000/2))
							if normalizeCCG:
								CCGaxis[i][j].set_ylabel('counts - normalized ('+str(np.sum(dfCCG[x][y]))+')', fontsize=6)
							else:
								CCGaxis[i][j].set_ylabel('counts', fontsize=6)
							CCGaxis[i][j].tick_params(labelsize=4, color='k')
							CCGaxis[i][j].set_title(str(x)+'-'+str(y),fontsize=10)
				figCCG.tight_layout()

				if not os.path.exists(self.__dir__+'/visCCGs'):
					os.makedirs(self.__dir__+'/visCCGs')
				figCCGpath = self.__dir__+'/visCCGs'+'/CCG'
				unitsListStr = [str(i) for i in unitsList]
				for i in unitsListStr:
					figCCGpath+=', '
					figCCGpath+=i
				figCCG.savefig(figCCGpath+'.eps')
				figCCG.savefig(figCCGpath+'.png')

				self.dfCCG = dfCCG
				break


			if "ISI" in featuresList:
				self.InterSI(bin_size=bin_sizeISI, window_size=window_sizeISI, normalize = True)
				plotsxticks = np.arange(0, window_sizeISI*1000, bin_sizeISI*1000)
				figISI, ISIaxis = plt.subplots(len(unitsList), 1)
				colorFlag = 0
				for i, x in enumerate(unitsList):
					mew, std, skew = self.ISIparamsDic[x][0], self.ISIparamsDic[x][1]**0.5, self.ISIparamsDic[x][2]
					s = 'rate = %0.1f Hz\nCV = %0.2f \nSkewness = %0.3f' % (1./mew,std/mew,skew)
					if len(unitsList)==1:
						ISIaxis.bar(plotsxticks, self.ISIDic[x], color = 'r', linewidth=0.5)
						ISIaxis.annotate(s,xy=(0.65, 0.8),xytext=None,xycoords='axes fraction',fontsize=16, color='k')
						ISIaxis.set_xlim([0,window_sizeISI*1000]) # milliseconds
						ISIaxis.tick_params(labelsize=10)
						ISIaxis.set_xlabel('ISI (ms)', fontsize=14)
						ISIaxis.set_ylabel('Count', fontsize=14)
					else:
						if colorFlag%5 == 0:
							color = (51./255, 153./255, 255./255)
						elif colorFlag%5 == 1:
							color = (255./255, 0./255, 0./255)
						elif colorFlag%5 == 2:
							color = (255./255, 255./255, 0./255)
						elif colorFlag%5 == 3:
							color = (0./255, 153./255, 0./255)
						elif colorFlag%5 == 4:
							color = (102./255, 0./255, 204./255)
						colorFlag+=1
						ISIaxis[i].bar(plotsxticks, self.ISIDic[x], color = color, linewidth=0.5)
						ISIaxis[i].annotate(s,xy=(0.65, 0.8),xytext=None,xycoords='axes fraction',fontsize=16)
						ISIaxis[i].set_xlim([0,window_sizeISI*1000]) # milliseconds
						ISIaxis[i].tick_params(labelsize=10)
						ISIaxis[i].set_xlabel('ISI (ms)', fontsize=14)
						ISIaxis[i].set_ylabel('Count', fontsize=14)
				figISI.tight_layout()
				if not os.path.exists(self.__dir__+'/visISIs'):
					os.makedirs(self.__dir__+'/visISIs')
				figISIpath = self.__dir__+'/visISIs'+'/ISI'
				unitsListStr = [str(i) for i in unitsList]
				for i in unitsListStr:
					figISIpath+=', '
					figISIpath+=i
				figISI.savefig(figISIpath+'.eps')
				figISI.savefig(figISIpath+'.png')
				break

			if "WVF" in featuresList:
				self.attribute_spikeTemplates()
				WVFDic = {unit: self.attributed_spikeTemplatesDic[unit] for unit in unitsList}

				dfWVF = pd.DataFrame(WVFDic)
				axWVF = dfWVF.plot(x_compat=True, colormap='cubehelix')
				axWVF.set_ylabel('??', fontsize=6)
				axWVF.set_xlabel('Time samples (30kHz)', fontsize=6)
				figWVF = axWVF.get_figure()
				if not os.path.exists(self.__dir__+'/visWVFs'):
					os.makedirs(self.__dir__+'/visWVFs')
				figWVFpath = self.__dir__+'/visWVFs'+'/WVF'
				unitsListStr = [str(i) for i in unitsList]
				for i in unitsListStr:
					figWVFpath+=', '
					figWVFpath+=i
				figWVF.savefig(figWVFpath+'.eps')
				figWVF.savefig(figWVFpath+'.png')

				self.dfWVF = dfWVF
				break


			if SHOW == True:
				#plt.show()
				pass
	
	def save_DM(self, filename=None):
		'''save() -> saving a DataManager() instance. Argument: filename, has to be of the form "xxxxxxx.pkl".'''
		if not os.path.exists(self.__dir__+'/saveCTC_dManager'):
			os.makedirs(self.__dir__+'/saveCTC_dManager')

		if filename==None:
			if not os.path.exists(self.__dir__+'/saveCTC_dManager'):
				os.makedirs(self.__dir__+'/saveCTC_dManager')
			filename=self.__dir__+'/saveCTC_dManager'+'/DataManager_'+time.strftime("%Y.%m.%d-%H:%M")+'.pkl'
		else:
			filename=self.__dir__+'/saveCTC_dManager'+'/'+filename
		with open(filename, 'wb') as output:
			dill.dump(self, output)



if __name__ == '__main__':
	data = DataManager()

'''
CCGCCG = np.zeros((505, 81))
for i in range(len(data.CCG)):
     for j in range(len(data.CCG[i])):
         if i==j:
             CCGCCG[i]+=data.CCG[i][j]
np.unique(np.where(np.isnan(CCGCCG))[0])'''

'''
import numpy as np
import matplotlib.pyplot as plt
import FeaturesExtraction as fext
data = fext.DataManager(2)
'''

''' -->> Fit ISI <<--
plt.close()
y = np.array(data.ISIListDic[4])*1000
x = np.arange(y.shape[0])
h = plt.hist(y, bins=range(400), color='w')

dists = ['alpha', 'lognorm']
for dist_name in dists:
	dist = getattr(scipy.stats, dist_name)
	param = dist.fit(y)
	pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * y.shape[0]
	plt.plot(pdf_fitted, label=dist, legend=True)

plt.xlim(0,400)
plt.show()

'''
