# -*- coding utf-8 -*-

# This script allows to use a scikit-learn.org classifier (supervised or unsupervised),
# in order to sort kilosort/phy generated units according to their cell type
# using their features, extracted thanks to the DataManager class written in FeaturesExtraction.py.

# Maxime Beau, 2017-05-10

import os, sys

#from CellTypeClassifier import FeaturesExtraction as fext
sys.path.append('./')
import FeaturesExtraction as fext

import numpy as np

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
#matplotlib.style.use('fivethirtyeight')
#matplotlib.style.use('ggplot')
matplotlib.style.use('classic')

import os, sys


import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, Normalizer, Imputer
from sklearn.pipeline import make_pipeline
from sklearn import datasets, svm, metrics


class CellTypeFinder():
    
    def __init__(self, directory=None, featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.001, window_sizeCCG=0.080, bin_sizeISI=0.0005):
        if directory==None:
            directory = os.getcwd()
        elif directory==1:
            directory='/Volumes/DK_students1/2017-04-08'
        elif directory==2:
            directory='/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/debugCTC'
        self.data = fext.DataManager(directory)
        self.data.extractFeatures(featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.001, window_sizeCCG=0.080, bin_sizeISI=0.0005)
    
    def preProFeatures(self, standardize=True, normalize=true):
        
        # Impute the missing data points -- NOT NEEDED, UNITS WITH NAN VALUES EXCLUDED
        #imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
        #imp.fit(self.data.extractedFeatures)
        #self.imputedFeatures = imp.transform(self.data.extractedFeatures)
        
        # Standardization: mean removal + variance scaling (divide by standard deviation)
        if standardize==True:
            scaler = StandardScaler()
            scaler.fit(self.data.extractedFeatures)
            StandardScaler(copy=True, with_mean=True, with_std=True)
            self.scaledFeatures = scaler.transform(self.data.extractedFeatures)
        
        # Normalising
        if normalize==True:
            normalizer=Normalizer()
            normalizer.fit(self.data.extractedFeatures)      
            self.normalizedFeatures = normalizer.transform(self.data.extractedFeatures)


    def clustering(self, unitsList='all', featuresList=None, n_clusters=10):
        '''Unsupervised learning to sort according to cellt type.'''
        allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]
            
        if (featuresList == None or featuresList == []) and EXIT == False:
            featuresList = []
            
            while 1:
                idx = input("\n\nPlease dial a feature to use for the clustering - MFRf0, CCGf0-5, ISIf0-2 or <all> for all features; dial <d> if you are done: ")
                if idx == "all":
                    featuresList = allfeaturesList
                    break
                elif idx == "d":
                    break
                elif idx in allfeaturesList:
                    featuresList.append(idx)
                else:
                    print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
        
            if featuresList==[]:
                print("\nYou didn't provide any feature. All features will then be used.")
                featuresList = allfeaturesList

        if unitsList=="all":
            unitsList=self.data.usedUnits
            


        print("\n\n--> Units to cluster: ", unitsList, "\n\n--> Features used for clustering: ", featuresList, "\n\n")
    
        # Select features used to feed the model
        featuresDataframe = pd.DataFrame(data=self.data.extractedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
        featuresMatrix = featuresDataframce.as_matrix(columns=featuresList)
        
        # Clustering: Kmeans algorithm
        model = KMeans(n_clusters=n_clusters)
        model.fit(featuresMatrix)
        self.clusteringLabels = model.predict(featuresMatrix)
        self.clusteringCentroids = model.cluster_centers_
        
        return featuresMatrix, labels
    
    
    
    def classifying(self):
        '''Supervised learning to sort according to cell type.'''


    def visualize(self, featuresList=None, unitsList="all", algo="Clustering"):


        if algo=="Clustering":
            self.preProFeatures()
            self.clustering()
        if algo=="Classifying":
            self.preProFeatures()
            self.clustering()

        allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]

        while 1:
            # Choose 2 or 3 features to display (in 2D or 3D)
            if (featuresList == None or featuresList == []) and EXIT == False:
                featuresList = []          
                while 1:
                    if len(featuresList)==3:
                        print("Three features provided, let's visualize your labels.")
                        break
                    idx = input("\n\nPlease dial 2 or three features to use for the clusters visualisation - MFRf0, CCGf0-5 or ISIf0-2; dial <d> if you are done: ")

                    if idx == "d":
                        if len(featuresList)<2:
                            print("You must at least provide 2 features. Only ", len(featuresList), " provided.")
                        else:
                            break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. You need to provide 2 or 3 features.")
                    break
            
            # Choose units to use
            if unitsList=="all":
                unitsList=self.data.usedUnits

            # Which algo was used: clustering or classifying
            if algo == "Clustering":
                labels = self.clusteringLabels
                centroids = self.clusteringCentroids
            elif algo == "Classifying":
                labels = self.classifyingLabels
                centroids = self.classifyingCentroids

            # Plot in 2D or 3D
            if len(featuresList)==2:
                labels_x = 
                labels_y = 
                colors = 
                centroids_x = 
                centroids_y = 
                plt.scatter(labels_x, labels_y, marker='o', color=colors)
                plt.scatter(centroids_x, centroids_y, marker='x', color='r')
            elif len(featuresList)==3:
                labels_x = 
                labels_y = 
                labels_z = 
                colors = 
                centroids_x = 
                centroids_y = 
                centroids_z = 
                plt.scatter(labels_x, labels_y, marker='o', color=colors)
                plt.scatter(centroids_x, centroids_y, marker='x', color='r')

####### ####### ####### ####### ####### ####### ####### ####### ####### #######  #######
# Features PREPROCESSING - each data point has to be linked to a >>1D<< features array #
####### ####### ####### ####### ####### ####### ####### ####### ####### #######  #######






####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######
#######             UNSUPERVISED LEARNING METHOD: Clustering, KMeans             ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######

### Standardize the features by making their variances equal
scaler = StandardScaler()
scaler.fit(xxxx)
StandardScaler(copy=True, with_mean=True, with_std=True)
xxxxScaled = scaler.transform(xxxx)
normalizer=Normalizer()


### Check where you're in an inertia elbow: k = 50 ?
inertias=[]
Kls = np.arange(10,100,5)
for k in Kls:
    kmeans = KMeans(n_clusters=20)
        pipeline = make_pipeline(scaler, normalizer, kmeans)
        pipeline.fit(xxxx)
        labels = pipeline.predict(xxxx)
        inertias.append(model.inertia_)
plt.plot(Kls, inertias)
plt.show()
plt.close()

### Find the clusters

kmeans = KMeans(n_clusters=20)
pipeline = make_pipeline(scaler, normalizer, kmeans)
pipeline.fit(xxxx)
labels = pipeline.predict(points)

### Plotting

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()














####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######
#######  SUPERVISED LEARNING METHOD: Classification, Support Vector Machine SVM  ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######

classifier = svm.SVC(gamma=0.001)

# data has to be [nparray1, nparray2, ...] and target [targetIndex1, targetIndex2...]
# where nparray1... are arrays of features. Can they be array of arrays ?
classifier.fit(data[], target[])



# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples / 2:]
predicted = classifier.predict(data[n_samples / 2:])

print("Classification report for classifier %s:\n%s\n"
      % (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    plt.subplot(2, 4, index + 5)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Prediction: %i' % prediction)

plt.show()

'''Personal notes: Clustering with Scikit learn
    https://www.youtube.com/watch?v=ZueoXMgCd1c&index=34&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v
    
    -
    -
    -
    -
    
    
    '''
