# -*- coding utf-8 -*-

# This script allows to use a scikit-learn.org classifier (supervised or unsupervised),
# in order to sort kilosort/phy generated units according to their cell type
# using their features, extracted thanks to the DataManager class written in FeaturesExtraction.py.

# Maxime Beau, 2017-05-10

import os, sys

#from CellTypeClassifier import FeaturesExtraction as fext
sys.path.append('./')
import FeaturesExtraction as fext

import numpy as np

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
#matplotlib.style.use('fivethirtyeight')
#matplotlib.style.use('ggplot')
matplotlib.style.use('classic')

import os, sys


import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, Normalizer, Imputer
from sklearn.pipeline import make_pipeline
from sklearn import datasets, svm, metrics


class CellTypeFinder():

    def __init__(self, directory=None, featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.001, window_sizeCCG=0.080, bin_sizeISI=0.0005):
        if directory==None:
            directory = os.getcwd()
        elif directory==1:
            directory='/Volumes/DK_students1/2017-04-08'
        elif directory==2:
            directory='/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/debugCTC'
        self.data = fext.DataManager(directory)
        self.data.extractFeatures(featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.001, window_sizeCCG=0.080, bin_sizeISI=0.0005)

    def preProFeatures(self):

        # Impute the missing data points -- NOT NEEDED, UNITS WITH NAN VALUES EXCLUDED
        #imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
        #imp.fit(self.data.extractedFeatures)
        #self.imputedFeatures = imp.transform(self.data.extractedFeatures)

        # Standardization: mean removal + variance scaling (divide by standard deviation)
        scaler = StandardScaler()
        scaler.fit(self.data.extractedFeatures)
        StandardScaler(copy=True, with_mean=True, with_std=True)
        self.scaledFeatures = scaler.transform(self.data.extractFeatures)

        # Normalising
        normalizer=Normalizer()

    def visFeatures(self, featuresList, unitsList='all'):
        if unitsList=='all':
            unitsList=self.data.units


    def clustering(self, unitsList=None, featuresList=None):
        '''Unsupervised learning to sort according to cellt type.'''
        EXIT = False
        while 1:
            self.extract_cIds()

            if unitsList == None or unitsList == []:
                unitsList = []

                while 1:
                    idx = input("\n\nPlease dial a cluster index ; dial <d> if you are done: ")

                    if idx == "d":
                        break
                    else:
                        try:
                            idx = int(idx) # input() only returns strings
                            if idx in self.clusters:
                                unitsList.append(idx)
                            else:
                                print("\nThis index is not detected in the cluster indexes of this directory's data. Try another one.")
                        except:
                            print("\nYou must dial a floatting point number or an integer.")

                if unitsList==[]:
                    print("\nYou didn't provide any unit. Not very wise, young (wo)man.")
                    EXIT = True

            if (featuresList == None or featuresList == []) and EXIT == False:
                featuresList = []

                while 1:
                    idx = input("\n\nPlease dial a feature to use for the clustering - ???; dial <d> if you are done: ")
                    
                    if idx == "d":
                        break
                    if idx == '?' or idx == '?' or idx == '?' or idx == '?':
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFR>, <IFR>, <CCG> or <WVF>.")

                if featuresList==[]:
                    print("\nYou didn't provide any feature. Not very wise, young (wo)man.")
                    EXIT = True

            if EXIT==True:
                print("\n\nGoodbye, thanks for using Max's tools.")
                break


            print("\n\n--> Units to visualize: ", unitsList, "\n\n--> Features displayed: ", featuresList, "\n\n")
            
        if 'MFR' in featuresList:
            self.MeanFR()
            break
        if 'IFR' in featuresList:
            self.InstFR()
            break
        if 'CCG' in featuresList:
            self.CrossCG()
            break
        if 'WVF' in featuresList:
            self.attribute_spikeTemplates()
            break




    def classifying(self):
        '''Supervised learning to sort according to cellt type.'''






####### ####### ####### ####### ####### ####### ####### ####### ####### #######  ####### 
# Features PREPROCESSING - each data point has to be linked to a >>1D<< features array #
####### ####### ####### ####### ####### ####### ####### ####### ####### #######  ####### 






####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 
#######             UNSUPERVISED LEARNING METHOD: Clustering, KMeans             ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 

### Standardize the features by making their variances equal
scaler = StandardScaler()
scaler.fit(xxxx)
StandardScaler(copy=True, with_mean=True, with_std=True)
xxxxScaled = scaler.transform(xxxx)
normalizer=Normalizer()


### Check where you're in an inertia elbow: k = 50 ?
inertias=[]
Kls = np.arange(10,100,5)
for k in Kls:
	kmeans = KMeans(n_clusters=20)
	pipeline = make_pipeline(scaler, normalizer, kmeans)
	pipeline.fit(xxxx)
	labels = pipeline.predict(xxxx)
	inertias.append(model.inertia_)
plt.plot(Kls, inertias)
plt.show()
plt.close()

### Find the clusters

kmeans = KMeans(n_clusters=20)
pipeline = make_pipeline(scaler, normalizer, kmeans)
pipeline.fit(xxxx)
labels = pipeline.predict(points)

### Plotting

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()














####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 
#######  SUPERVISED LEARNING METHOD: Classification, Support Vector Machine SVM  ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 

classifier = svm.SVC(gamma=0.001)

# data has to be [nparray1, nparray2, ...] and target [targetIndex1, targetIndex2...]
# where nparray1... are arrays of features. Can they be array of arrays ?
classifier.fit(data[], target[]) 



# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples / 2:]
predicted = classifier.predict(data[n_samples / 2:])

print("Classification report for classifier %s:\n%s\n"
      % (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    plt.subplot(2, 4, index + 5)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Prediction: %i' % prediction)

plt.show()

'''Personal notes: Clustering with Scikit learn
https://www.youtube.com/watch?v=ZueoXMgCd1c&index=34&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v

-
-
-
-


'''
