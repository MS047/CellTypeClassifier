# -*- coding utf-8 -*-

# This script allows to use a scikit-learn.org classifier (supervised or unsupervised),
# in order to sort kilosort/phy generated units according to their cell type
# using their features, extracted thanks to the DataManager class written in FeaturesExtraction.py.

# Maxime Beau, 2017-05-10

import os, os.path, sys
import time

sys.path.append('./')
import FeaturesExtraction as fext

import numpy as np

import pandas as pd
from pandas.tools.plotting import table
import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
#matplotlib.style.use('fivethirtyeight')
#matplotlib.style.use('ggplot')
matplotlib.style.use('classic')

import sklearn
from sklearn.cluster import KMeans, SpectralClustering
from sklearn.preprocessing import StandardScaler, Normalizer, Imputer
from sklearn.pipeline import make_pipeline
from sklearn import datasets, svm, metrics
from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors
from sklearn.model_selection import train_test_split



class CellTypeFinder():
    
    def __init__(self, directory=None, featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=0.001, window_sizeCCG=0.1, bin_sizeISI=0.0005):
        if directory==None:
            directory = os.getcwd()
        elif directory==1:
            directory='/Volumes/DK_students1/2017-04-08'
        elif directory==2:
            directory='/Users/maximebeau/Desktop/Science/5_Master_2/Internship/Data_Analysis/debugCTC'
        print("\n>>> Extraction of the dataset features:")
        self.data = fext.DataManager(directory)
        self.data.extractFeatures(featuresList=['MFR','CCG', 'ISI', 'WVF'],  bin_sizeCCG=bin_sizeCCG, window_sizeCCG=window_sizeCCG, bin_sizeISI=0.0005)
    
    def preProcessFeatures(self, standardize=True, normalize=True):
        
        # Impute the missing data points -- NOT NEEDED, UNITS WITH NAN VALUES EXCLUDED BEFOREHAND
        #imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
        #imp.fit(self.data.extractedFeatures)
        #self.imputedFeatures = imp.transform(self.data.extractedFeatures)

        # Standardization: mean removal + variance scaling (divide by standard deviation)
        if standardize==True:
            scaler = StandardScaler()
        # Normalizing
        if normalize==True:
            normalizer=Normalizer()

        # Make the resulting pipeline
        if standardize==True and normalize==False:
            pipeline = make_pipeline(scaler)
        elif standardize==False and normalize==True:
            pipeline = make_pipeline(normalizer)
        elif standardize==True and normalize==True:
            pipeline = make_pipeline(scaler, normalizer)

        # Fit the pipeline to the model + Return the data processed through the fitted pipeline
        self.preProcessedFeatures = pipeline.fit_transform(self.data.extractedFeatures)
        
        return self.preProcessedFeatures


    def clustering(self, unitsList='all', featuresList=None, algo="K-means", n_clust_range=[4,5,6,7,8,9,10,11,12,13],
                    algoParams={'n_clusters':10, 'init':'k-means++', 'n_init':10, 'max_iter':300, 'tol':0.0001, 'precompute_distances':'auto', 'verbose':0, 'random_state':None, 'copy_x':True, 'n_jobs':1, 'algorithm':'auto'}, 
                    standardize=True, normalize=True, again=False):
        '''Unsupervised learning to sort according to cell type. Kmeans algorithm.'''

        try:
            print("Units already clustered.\n", self.clusteringFeaturesDF)
            if again==True:
                if input(" -- Cluster again? Dial <anything> for yes, <enter> for no: "):
                    raise

        except:
            ## Preprocess data if required
            if standardize==True or normalize==True:
                self.preProcessFeatures(standardize=standardize, normalize=normalize)


            ## Select features to use
            allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]  
            if (featuresList == None or featuresList == []):
                featuresList = []
                
                while 1:
                    idx = input("\n\nPlease dial a feature to use for the clustering - MFRf0, CCGf0-5, ISIf0-2 or <all> for all features; dial <d> if you are done: ")
                    if idx == "all":
                        featuresList = allfeaturesList
                        break
                    elif idx == "d":
                        break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. All features will then be used.")
                    featuresList = allfeaturesList

            if unitsList=="all":
                allunits=True
                unitsList=self.data.usedUnits   

            print("\n\n--> Units to cluster: ", "all" if allunits==True else unitsList, "\n\n--> Features used for clustering: ", featuresList, "\n\n")
        
            if standardize==True or normalize==True:
                featuresDataframe = pd.DataFrame(data=self.preProcessedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            else:
                featuresDataframe = pd.DataFrame(data=self.data.extractedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            self.clusteringFeaturesDF = featuresDataframe.filter(featuresList, axis=1)
            featuresMatrix = self.clusteringFeaturesDF.as_matrix()



            ## Clustering

            #First, pick the best number of cluster with inertia as readout
            inertias = []
            for N_CLUSTERS in n_clust_range:
                algoParams['n_clusters']=N_CLUSTERS
                if algo=="K-means":
                    model = KMeans(**algoParams)       
                if algo=="SpectralClustering":
                    model = SpectralClustering(**algoParams)
                
                model.fit(featuresMatrix)    
                inertias.append(model.inertia_)

            showPlot=True
            while 1:
                if showPlot==True:
                    fig = plt.figure()
                    ax = fig.add_subplot(111)
                    ax.plot(n_clust_range, inertias, '-o')
                    ax.set_xlabel('n_clusters', fontsize=12)
                    ax.set_ylabel('inertia (Sum of distances of samples to their closest cluster centroid.)', fontsize=8)
                    fig.suptitle('Choose n_clusters at the point where the inertia profile presents an elbow', fontsize=12, fontweight='bold')
                    ax.set_title('Aims the lowest inertia possible while having a reasonably low number of clusters.', fontsize=8)
                    plt.show()
                    showPlot=False
                BEST_N_CLUSTERS = int(input("Pick the best n_clusters, i.e. the elbow of the inertias graph, or <0> to see the plot again: "))
                if BEST_N_CLUSTERS == 0:
                    showPlot=True
                elif BEST_N_CLUSTERS in n_clust_range:
                    print("You picked <", BEST_N_CLUSTERS, "> as n_clusters.")
                    algoParams['n_clusters']=BEST_N_CLUSTERS
                    break
                else:
                    print("You must dial one of the tested n_clusters, in this list:", n_clust_range)

            # Pick a model, with your n_cluster as argument
            if algo=="K-means":
                model = KMeans(**algoParams)       
            if algo=="SpectralClustering":
                model = SpectralClustering(**algoParams)
            if algo=="Nearest-Neighbors": # Not tested
                model = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')            
            # Fit your model to the unLabeled data
            model.fit(featuresMatrix)  

            # Extract the labels your model have given to the datapoints  
            clusteringLabels = model.predict(featuresMatrix)
            self.clusteringFeaturesDF['Labels']=pd.Series(clusteringLabels, index=self.clusteringFeaturesDF.index)
            self.clusteringCentroids = model.cluster_centers_

            
            return self.clusteringFeaturesDF
    
    
    
    def classifying(self, unitsList='all', featuresList=None, algo="K-nearest-neighbors", n_clust_range=[4,5,6,7,8,9,10,11,12,13],
                    algoParams={'n_neighbors':5, 'weights':'uniform', 'algorithm':'auto', 'leaf_size':30, 'p':2, 'metric':'minkowski', 'metric_params':None, 'n_jobs':1,}, 
                    standardize=True, normalize=True, again=False):
        '''Supervised learning to sort according to cell type. Algorithm Single Vector Machine.'''

        try:
            print("Units already classified.\n", self.classifyingFeaturesDF)
            if again==True:
                if input(" -- Classify again? Dial <anything> for yes, <enter> for no: "):
                    raise

        except:
            ## Preprocess data if required
            if standardize==True or normalize==True:
                self.preProcessFeatures(standardize=standardize, normalize=normalize)


            ## Select features to use
            allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]  
            if (featuresList == None or featuresList == []):
                featuresList = []
                
                while 1:
                    idx = input("\n\nPlease dial a feature to use for the clustering - MFRf0, CCGf0-5, ISIf0-2 or <all> for all features; dial <d> if you are done: ")
                    if idx == "all":
                        featuresList = allfeaturesList
                        break
                    elif idx == "d":
                        break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. All features will then be used.")
                    featuresList = allfeaturesList

            if unitsList=="all":
                allunits=True
                unitsList=self.data.usedUnits   

            print("\n\n--> Units to classify: ", "all" if allunits==True else unitsList, "\n\n--> Features used for classifying: ", featuresList, "\n\n")
        
            if standardize==True or normalize==True:
                featuresDataframe = pd.DataFrame(data=self.preProcessedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            else:
                featuresDataframe = pd.DataFrame(data=self.data.extractedFeatures, index=self.data.usedUnits, columns = allfeaturesList)
            self.classifyingFeaturesDF = featuresDataframe.filter(featuresList, axis=1)
            featuresMatrix = self.classifyingFeaturesDF.as_matrix()



            ## Classifying

            # import preLabeled data (labeled by eye (08/05/2017) or optogenetics). 1D numpy array or text file.
            #preLabData = np.load(preLabeledData.npy)
            #preLabData = np.loadtxt(preLabeledData.txt)
            
            # Pick a model
            if algo=="K-nearest-neighbors":
                model = KNeighborsClassifier(**algoParams)       
            if algo=="Support-Vector-Machine":
                model = svm.SVC(**algoParams)

            # Fit the model to the train preLabeled data + predict the test preLabeled data labels, to compute accuracy.
            # random_state: seeds the split, stratify: allows to take into account ditribution of features to make train and test data equivalent.
            preLabData_train, preLabData_test, labels_train, labels_test = train_test_split(preLabData, labels, test_size=0.3, random_state=21, stratify=labels)
            model.fit(preLabData_train, labels_train)
            accuracy = model.score(preLabData_test, labels_test)
            print("Accuracy of the classifier: {}".format(accuracy))
            # Fit the model to the whole preLabeled data.
            model.fit(preLabData)
            # Predict the labels of the unLabeled data.
            classifyingLabels = model.predict(featuresMatrix)
            self.classifyingFeaturesDF['Labels']=pd.Series(clusteringLabels, index=self.clusteringFeaturesDF.index)
            self.classifyingCentroids = model.cluster_centers_
            
            return self.classifyingFeaturesDF


    def inspect_labels(self, featuresList=None, unitsList="all", algo=["Clustering", "K-means"], standardize=True, normalize=True, again=False):

        allfeaturesList = ["MFRf0", "CCGf0", "CCGf1", "CCGf2", "CCGf3", "CCGf4", "CCGf5", "ISIf0", "ISIf1", "ISIf2"]
        TIME = time.strftime("%Y.%m.%d-%H.%M")

        ## Choose 2 or 3 features to display (in 2D or 3D)
        while 1:
            if (featuresList == None or featuresList == []):
                featuresList = []          
                while 1:
                    if len(featuresList)==3:
                        print("Three features already provided, time to visualize labels.")
                        break
                    idx = input("\n\nPlease dial 2 or three features to use for the clusters visualization - MFRf0, CCGf0-5 or ISIf0-2; dial <d> if you are done: ")

                    if idx == "d":
                        if len(featuresList)<2:
                            print("You must at least provide 2 features. Only ", len(featuresList), " provided.")
                        else:
                            break
                    elif idx in allfeaturesList:
                        featuresList.append(idx)
                    else:
                        print("\nYou must dial <MFRf0>, <CCGf0>, <CCGf1>, <CCGf2>, <CCGf3>, <CCGf4>, <CCGf5>, <ISIf0>, <ISIf1> or <ISIf2>.")
            
                if featuresList==[]:
                    print("\nYou didn't provide any feature. You need to provide 2 or 3 features.")
                    break
            
            print("\n\n --> Features to plot: ", featuresList)


            ## Choose units to use
            if unitsList=="all":
                unitsList=self.data.usedUnits


            ## Which algo was used: clustering or classifying
            if algo[0]=="Clustering":
                self.clustering(algo=algo[1], standardize=standardize, normalize=normalize, again=again)
                data = self.clusteringFeaturesDF.drop('Labels', axis=1, inplace=False)
                labels = self.clusteringFeaturesDF['Labels'].tolist()
                centroids = self.clusteringCentroids
            elif algo[0]=="Classifying":
                self.classifying(algo=algo[1], standardize=standardize, normalize=normalize, again=again)
                data = self.classifyingFeaturesDF.drop('Labels', axis=1, inplace=False)
                labels = self.classifyingFeaturesDF['Labels'].tolist()
                centroids = self.classifyingCentroids


            ## Cross-tabulation with expected labels (from eye first, then with ground truth data)
            # Add the labeled celltypes to the DataFrame
            cellTypes = np.load()
            self.classifyingFeaturesDF['cellTypes'] = pd.Series(cellTypes, index=self.clusteringFeaturesDF.index)

            # Create crosstab: ct
            ct = pd.crosstab(self.classifyingFeaturesDF['Labels'], self.classifyingFeaturesDF['cellTypes'])

            # Save it as eps/png
            fig = plt.figure()
            ax = fig.add_subplot(111, frame_on=False)# no visible frame
            ax.xaxis.set_visible(False)  # hide the x axis
            ax.yaxis.set_visible(False)  # hide the y axis
            table(ax, ct)  # where df is your data frame
            if not os.path.exists(self.data.__dir__+'/ClusteringResults'):
                os.makedirs(self.data.__dir__+'/ClusteringResults')
            figPath = self.data.__dir__+'/ClusteringResults/'+TIME+'_Cross-Tabulation'
            fig.savefig(figPath+'.eps')
            fig.savefig(figPath+'.png')


            ## Exploratory Data Analysis

            # Plot scatter matrix
            scatterMatrix = pd.scatter_matrix(data.drop('Labels', axis=1), c=data['Labels'].tolist(), figsize=[20,20], s=150, marker='o', alpha=0.5)
            scatterMatrix.suptitle("Scatter Matrix of clustering features", fontsize=14, fontweight='bold')
            if not os.path.exists(self.data.__dir__+'/ClusteringResults'):
                os.makedirs(self.data.__dir__+'/ClusteringResults')
            figPath = self.data.__dir__+'/ClusteringResults/'+TIME+'_Scatter-Matrix'
            fig.savefig(figPath+'.eps')
            fig.savefig(figPath+'.png')

            # Plot 2 features
            if len(featuresList)==2:
                x = data[featuresList[0]].tolist()
                y = data[featuresList[1]].tolist()
                idx_x = data.columns.get_loc(featuresList[0])
                idx_y = data.columns.get_loc(featuresList[1])
                centroids_x = centroids[:,idx_x]
                centroids_y = centroids[:,idx_y]

                fig = plt.figure()
                ax = fig.add_subplot(111)
                ax.scatter(x, y, c=labels, alpha=0.5)
                ax.scatter(centroids_x, centroids_y, marker='D', color='r')
                ax.set_xlabel(str(featuresList[0]), fontsize=10)
                ax.set_ylabel(str(featuresList[1]), fontsize=10)
                ax.set_title("Clustering results - "+str(algo[1]), fontsize=14, fontweight='bold')

                if not os.path.exists(self.data.__dir__+'/ClusteringResults'):
                    os.makedirs(self.data.__dir__+'/ClusteringResults')
                figPath = self.data.__dir__+'/ClusteringResults/'+TIME+'_'+str(algo[1])+'-Feat: '+str(featuresList[0])+', '+str(featuresList[1])
                fig.savefig(figPath+'.eps')
                fig.savefig(figPath+'.png')
                plt.show()
                break

            # Plot 3 features
            elif len(featuresList)==3:
                x = data[featuresList[0]].tolist()
                y = data[featuresList[1]].tolist()
                z = data[featuresList[2]].tolist()
                idx_x = data.columns.get_loc(featuresList[0])
                idx_y = data.columns.get_loc(featuresList[1])
                idx_z = data.columns.get_loc(featuresList[2])
                centroids_x = centroids[:,idx_x]
                centroids_y = centroids[:,idx_y]
                centroids_z = centroids[:,idx_z]

                fig = plt.figure()
                ax = fig.add_subplot(111, projection='3d')
                ax.scatter(xs=x, ys=y, zs=z, c=labels, alpha=0.5)
                ax.scatter(xs=centroids_x, ys=centroids_y, zs=centroids_z, marker='D', color='r')
                ax.set_xlabel(str(featuresList[0]), fontsize=10)
                ax.set_ylabel(str(featuresList[1]), fontsize=10)
                ax.set_zlabel(str(featuresList[2]), fontsize=10)
                ax.set_title("Clustering results - "+str(algo[1]), fontsize=14, fontweight='bold')
                plt.show()
                break


'''



####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######
#######  SUPERVISED LEARNING METHOD: Classification, Support Vector Machine SVM  ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### #######

classifier = svm.SVC(gamma=0.001)

# data has to be [nparray1, nparray2, ...] and target [targetIndex1, targetIndex2...]
# where nparray1... are arrays of features. Can they be array of arrays ?
classifier.fit(data[], target[])



# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples / 2:]
predicted = classifier.predict(data[n_samples / 2:])

print("Classification report for classifier %s:\n%s\n"
      % (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    plt.subplot(2, 4, index + 5)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Prediction: %i' % prediction)

plt.show()'''



'''Personal notes: Clustering with Scikit learn
    https://www.youtube.com/watch?v=ZueoXMgCd1c&index=34&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v
    
    -
    -
    -
    -
    
    '''

