# -*- coding utf-8 -*-

# This script allows to use a scikit-learn.org classifier (supervised or unsupervised),
# in order to sort kilosort/phy generated units according to their cell type
# using their features, extracted thanks to the DataManager class written in FeaturesExtraction.py.

# Maxime Beau, 2017-05-10

import os, sys

from CellTypeClassifier import FeaturesExtraction as fe

import numpy as np

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
matplotlib.style.use('ggplot')

import os, sys


import sklearn
from sklearn.cluster import KMeans
from sklearn import datasets, svm, metrics

####### ####### ####### ####### ####### ####### ####### ####### ####### #######  ####### 
# Features PREPROCESSING - each data point has to be linked to a >>1D<< features array #
####### ####### ####### ####### ####### ####### ####### ####### ####### #######  ####### 

data = fe.DataManager()
data.MFR()
data.IFR()
data.CCG(bin_size=0.0005, window_size=0.080)

proccessedFeatures = []


## Already ok for the MFR, one number
for x in data.MFR:
	proccessedFeatures.append([x[1]])


## Sort cells in classes according to their IFR to summarize the IFR in one number
IFRmodel = KMeans(n_clusters=50)
IFRmodel.fit(data.IFR[:,1:])
IFRlabels = model.predict(data.IFR[:,1:])
IFRclasses = [np.array([data.IFR[i][0], IFRlabels[i]]) for i in range(len(IFRlabels))]
# IFRclasses: [np.array([cluster1, IFRclass of cluster1]), np.array([cluster2, IFRclass of cluster2])...]
for i, x in enumerate(proccessedFeatures):
	x.append(IFRclasses[i][1])


## Sort cells in classes according to their CCG to summarize the CCG in one number
CCGmodel = KMeans(n_clusters=50)
CCGmodel.fit(data.CCG[:,1:])
CCGlabels = model.predict(data.CCG[:,1:])
CCGclasses = [np.array([data.CCG[i][0], CCGlabels[i]]) for i in range(len(CCGlabels))]
# IFRclasses: [np.array([cluster1, IFRclass of cluster1]), np.array([cluster2, IFRclass of cluster2])...]
for i, x in enumerate(proccessedFeatures):
	x.append(CCGclasses[i][1])







####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 
#######             UNSUPERVISED LEARNING METHOD: Clustering, KMeans             ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 

### Finding categories

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=20)

# Fit model to points
model.fit(points)

# Determine the cluster labels of new_points: labels
labels = model.predict(new_points)

# Print cluster labels of new_points
print(labels)


### Plotting

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()














####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 
#######  SUPERVISED LEARNING METHOD: Classification, Support Vector Machine SVM  ######
####### ####### ####### ####### ####### ####### ####### ####### ####### ####### ####### 

classifier = svm.SVC(gamma=0.001)

# data has to be [nparray1, nparray2, ...] and target [targetIndex1, targetIndex2...]
# where nparray1... are arrays of features. Can they be array of arrays ?
classifier.fit(data[], target[]) 



# The digits dataset
digits = datasets.load_digits()

# The data that we are interested in is made of 8x8 images of digits, let's
# have a look at the first 4 images, stored in the `images` attribute of the
# dataset.  If we were working from image files, we could load them using
# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
# images, we know which digit they represent: it is given in the 'target' of
# the dataset.
images_and_labels = list(zip(digits.images, digits.target))
for index, (image, label) in enumerate(images_and_labels[:4]):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Training: %i' % label)

# To apply a classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# Create a classifier: a support vector classifier
classifier = svm.SVC(gamma=0.001)

# We learn the digits on the first half of the digits
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])

# Now predict the value of the digit on the second half:
expected = digits.target[n_samples / 2:]
predicted = classifier.predict(data[n_samples / 2:])

print("Classification report for classifier %s:\n%s\n"
      % (classifier, metrics.classification_report(expected, predicted)))
print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))

images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    plt.subplot(2, 4, index + 5)
    plt.axis('off')
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Prediction: %i' % prediction)

plt.show()


